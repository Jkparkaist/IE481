{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    ''''\n",
    "    -------------------------------------------------------------------------------\n",
    "    Filename   : ch3.ipynb\n",
    "    Date       : 2012-06-17\n",
    "    Author     : C. Vogel\n",
    "    Purpose    : Replicate the naive Bayes e-mail classifier in Chapter 3 of \n",
    "               : _Machine Learning for Hackers_.\n",
    "    Input Data : e-mail files, split into spam and ham (non-spam) folders are available \n",
    "               : at the book's github repository at https://github.com/johnmyleswhite/\n",
    "               : ML_for_Hackers.git. This also uses r_stopwords.csv, a text file \n",
    "               : containing a list of stopwords used by R's tm package. This is used\n",
    "               : to facilitate comparability with the results of the R analysis.\n",
    "    Libraries  : Numpy 1.6.1, Pandas 0.7.3, NLTK 2.0.1, textmining\n",
    "    -------------------------------------------------------------------------------\n",
    "\n",
    "    This notebook is a Python port of the R code in Chapter 3 of _Machine Learning\n",
    "    for Hackers_ by D. Conway and J.M. White.\n",
    "\n",
    "    E-mail files, split into folders classified as spam or ham (non-spam) should be located \n",
    "    in a /data/ subfolder of the working directory. See the paths defined just after the import\n",
    "    statements below to see what directory structure this script requires. Copying complete\n",
    "    data folder from the book's github repository should be sufficient.\n",
    "\n",
    "    For a detailed description of the analysis and the process of porting it\n",
    "    to Python, see: slendrmeans.wordpress.com/will-it-python.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import textmining as txtm\n",
    "from pandas import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Directories with e-mail data\n",
    "# The spam and ham files are broken into multiple \n",
    "# directories so as to separate training and evaluation data\n",
    "data_path = os.path.abspath(os.path.join('.', 'data'))\n",
    "spam_path = os.path.join(data_path, 'spam')\n",
    "spam2_path = os.path.join(data_path, 'spam_2') \n",
    "easyham_path = os.path.join(data_path, 'easy_ham')\n",
    "easyham2_path = os.path.join(data_path, 'easy_ham_2')\n",
    "hardham_path = os.path.join(data_path, 'hard_ham')\n",
    "hardham2_path = os.path.join(data_path, 'hard_ham_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_msg(path):\n",
    "    '''\n",
    "    Read in the `message` portion of an e-mail, given\n",
    "    its file path. The `message` text begins after the first\n",
    "    blank line; above is header information.\n",
    "\n",
    "    Returns a string.\n",
    "    '''\n",
    "    with open(path, 'rU') as con:\n",
    "        msg = con.readlines()\n",
    "        first_blank_index = msg.index('\\n')\n",
    "        msg = msg[(first_blank_index + 1): ]\n",
    "        return ''.join(msg)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_msgdir(path):\n",
    "    '''\n",
    "    Read all messages from files in a directory into\n",
    "    a list where each item is the text of a message. \n",
    "    \n",
    "    Simply gets a list of e-mail files in a directory,\n",
    "    and iterates `get_msg()` over them.\n",
    "\n",
    "    Returns a list of strings.\n",
    "    '''\n",
    "    filelist = os.listdir(path)\n",
    "    filelist = filter(lambda x: x != 'cmds', filelist)\n",
    "    all_msgs =[get_msg(os.path.join(path, f)) for f in filelist]\n",
    "    return all_msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get lists containing messages of each type.\n",
    "all_spam = get_msgdir(spam_path)\n",
    "all_easyham = get_msgdir(easyham_path)\n",
    "all_easyham = all_easyham[:500]\n",
    "all_hardham = get_msgdir(hardham_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get stopwords.\n",
    "# NLTK stopwords\n",
    "sw = stopwords.words('english')\n",
    "# Stopwords exported from the 'tm' library in R.\n",
    "rsw = read_csv('r_stopwords.csv')['x'].values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tdm_df(doclist, stopwords = [], remove_punctuation = True, \n",
    "           remove_digits = True, sparse_df = False):\n",
    "    '''\n",
    "    Create a term-document matrix from a list of e-mails.\n",
    "    \n",
    "    Uses the TermDocumentMatrix function in the `textmining` module.\n",
    "    But, pre-processes the documents to remove digits and punctuation,\n",
    "    and post-processes to remove stopwords, to match the functionality \n",
    "    of R's `tm` package.\n",
    "\n",
    "    NB: This is not particularly memory efficient and you can get memory \n",
    "    errors with an especially long list of documents.\n",
    "\n",
    "    Returns a (by default, sparse) DataFrame. Each column is a term,\n",
    "    each row is a document.\n",
    "    '''\n",
    "    \n",
    "    # Create the TDM from the list of documents.\n",
    "    tdm = txtm.TermDocumentMatrix()\n",
    "  \n",
    "    for doc in doclist:\n",
    "        if remove_punctuation == True:\n",
    "            doc = doc.translate(None, string.punctuation.translate(None, '\"'))\n",
    "        if remove_digits == True:\n",
    "            doc = doc.translate(None, string.digits)\n",
    "            \n",
    "        tdm.add_doc(doc)\n",
    "    \n",
    "    # Push the TDM data to a list of lists,\n",
    "    # then make that an ndarray, which then\n",
    "    # becomes a DataFrame.\n",
    "    tdm_rows = []\n",
    "    for row in tdm.rows(cutoff = 1):\n",
    "        tdm_rows.append(row)\n",
    "        \n",
    "    tdm_array = np.array(tdm_rows[1:])\n",
    "    tdm_terms = tdm_rows[0]\n",
    "    df = DataFrame(tdm_array, columns = tdm_terms)\n",
    "    \n",
    "    # Remove stopwords from the dataset, manually.\n",
    "    # TermDocumentMatrix does not do this for us.\n",
    "    if len(stopwords) > 0:\n",
    "        for col in df:\n",
    "            if col in stopwords:\n",
    "                del df[col]\n",
    "    \n",
    "    if sparse_df == True:\n",
    "        df.to_sparse(fill_value = 0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_tdm = tdm_df(all_spam, stopwords = rsw, sparse_df = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_term_df(tdm):\n",
    "    '''\n",
    "    Create a DataFrame that gives statistics for each term in a \n",
    "    Term Document Matrix.\n",
    "\n",
    "    `frequency` is how often the term occurs across all documents.\n",
    "    `density` is frequency normalized by the sum of all terms' frequencies.\n",
    "    `occurrence` is the percent of documents that a term appears in.\n",
    "\n",
    "    Returns a DataFrame, with an index of terms from the input TDM.\n",
    "    '''\n",
    "    term_df = DataFrame(tdm.sum(), columns = ['frequency'])\n",
    "    term_df['density'] = term_df.frequency / float(term_df.frequency.sum())\n",
    "    term_df['occurrence'] = tdm.apply(lambda x: np.sum((x > 0))) / float(tdm.shape[0])\n",
    "    \n",
    "    return term_df.sort_index(by = 'occurrence', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_term_df = make_term_df(spam_tdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>density</th>\n",
       "      <th>occurrence</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      <td><strong>email</strong></td>\n",
       "      <td> 868</td>\n",
       "      <td> 0.005907</td>\n",
       "      <td> 0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><strong>please</strong></td>\n",
       "      <td> 453</td>\n",
       "      <td> 0.003083</td>\n",
       "      <td> 0.520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><strong>click</strong></td>\n",
       "      <td> 370</td>\n",
       "      <td> 0.002518</td>\n",
       "      <td> 0.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><strong>list</strong></td>\n",
       "      <td> 422</td>\n",
       "      <td> 0.002872</td>\n",
       "      <td> 0.444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><strong>body</strong></td>\n",
       "      <td> 420</td>\n",
       "      <td> 0.002858</td>\n",
       "      <td> 0.410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        frequency   density  occurrence\n",
       "email         868  0.005907       0.576\n",
       "please        453  0.003083       0.520\n",
       "click         370  0.002518       0.450\n",
       "list          422  0.002872       0.444\n",
       "body          420  0.002858       0.410"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_term_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "easyham_tdm = tdm_df(all_easyham, stopwords = rsw, sparse_df = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "easyham_term_df = make_term_df(easyham_tdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>density</th>\n",
       "      <th>occurrence</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      <td><strong>wrote</strong></td>\n",
       "      <td> 237</td>\n",
       "      <td> 0.004052</td>\n",
       "      <td> 0.378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><strong>list</strong></td>\n",
       "      <td> 248</td>\n",
       "      <td> 0.004240</td>\n",
       "      <td> 0.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><strong>dont</strong></td>\n",
       "      <td> 241</td>\n",
       "      <td> 0.004120</td>\n",
       "      <td> 0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><strong>email</strong></td>\n",
       "      <td> 188</td>\n",
       "      <td> 0.003214</td>\n",
       "      <td> 0.276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><strong>subject</strong></td>\n",
       "      <td> 162</td>\n",
       "      <td> 0.002770</td>\n",
       "      <td> 0.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><strong>time</strong></td>\n",
       "      <td> 188</td>\n",
       "      <td> 0.003214</td>\n",
       "      <td> 0.258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         frequency   density  occurrence\n",
       "wrote          237  0.004052       0.378\n",
       "list           248  0.004240       0.368\n",
       "dont           241  0.004120       0.290\n",
       "email          188  0.003214       0.276\n",
       "subject        162  0.002770       0.270\n",
       "time           188  0.003214       0.258"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easyham_term_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_email(msg, training_df, prior = 0.5, c = 1e-6):\n",
    "    '''\n",
    "    A conditional probability calculator for a naive Bayes e-mail\n",
    "    classifier.\n",
    "    Given an e-mail message and a training dataset, the classifier\n",
    "    returns the log probability of observing the terms in the message if\n",
    "    it were of the same class as the e-mails in the training set (spam/ham).\n",
    "\n",
    "    NB: Log probabilities are used for this function, because the raw probabilities\n",
    "    will be so small that underflow is a real risk. Calculating probability\n",
    "    would require multiplying many occurrence probabilities -- p1 * p2 * ... * pN,\n",
    "    where pi is often ~= 0. For log probability we can compute ln(p1) + ln(p2) +\n",
    "    ... + ln(pN), where ln(pi) < 0 by a far. This will not affect the ordering \n",
    "    of probabilities (which is what we care about ultimately), but solves the \n",
    "    underflow risk. Cf. p. 89 of MLFH to see how small raw probability calculations\n",
    "    can get, and an apparent underflow in row 4.\n",
    "\n",
    "    Returns a log probability (float) between -Infty and +Infty.\n",
    "    '''\n",
    "    msg_tdm = tdm_df([msg])\n",
    "    msg_freq = msg_tdm.sum()\n",
    "    msg_match = list(set(msg_freq.index).intersection(set(training_df.index)))\n",
    "    if len(msg_match) < 1:\n",
    "        return math.log(prior) + math.log(c) * len(msg_freq)\n",
    "    else:\n",
    "        match_probs = training_df.occurrence[msg_match]\n",
    "        return (math.log(prior) + np.log(match_probs).sum() \n",
    "                + math.log(c) * (len(msg_freq) - len(msg_match)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hardham_spamtest = [classify_email(m, spam_term_df) for m in all_hardham]\n",
    "hardham_hamtest = [classify_email(m, easyham_term_df) for m in all_hardham]\n",
    "s_spam = np.array(hardham_spamtest) > np.array(hardham_hamtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spam_classifier(msglist):\n",
    "    '''\n",
    "    The naive Bayes classifier. \n",
    "    Using spam and ham training datasets, use `classify_email()` to\n",
    "    compute the conditional log probability of each e-mail in a list. \n",
    "    Assign each e-mail to whichever class's training data returns the \n",
    "    highest probability.\n",
    "\n",
    "    Returns a DataFrame with the conditional log probabilities and the\n",
    "    class.\n",
    "    '''\n",
    "    spamprob = [classify_email(m, spam_term_df) for m in msglist]\n",
    "    hamprob = [classify_email(m, easyham_term_df) for m in msglist]\n",
    "    classify = np.where(np.array(spamprob) > np.array(hamprob), 'Spam', 'Ham')\n",
    "    out_df = DataFrame({'pr_spam' : spamprob,\n",
    "                        'pr_ham'  : hamprob, \n",
    "                        'classify'   : classify}, \n",
    "                       columns = ['pr_spam', 'pr_ham', 'classify'])\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def class_stats(df):\n",
    "    return df.classify.value_counts() / float(len(df.classify))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spam    0.702811\n",
       "Ham     0.297189"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hardham_classify = spam_classifier(all_hardham)\n",
    "class_stats(hardham_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>pr_spam</th>\n",
       "      <th>pr_ham</th>\n",
       "      <th>classify</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    <tr>\n",
       "      <td><strong>0</strong></td>\n",
       "      <td>-3620.438622</td>\n",
       "      <td>-3628.335856</td>\n",
       "      <td> Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><strong>1</strong></td>\n",
       "      <td>-4961.487090</td>\n",
       "      <td>-5215.082391</td>\n",
       "      <td> Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><strong>2</strong></td>\n",
       "      <td> -374.143783</td>\n",
       "      <td> -393.897628</td>\n",
       "      <td> Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><strong>3</strong></td>\n",
       "      <td>-3190.000772</td>\n",
       "      <td>-3192.233073</td>\n",
       "      <td> Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><strong>4</strong></td>\n",
       "      <td>-9315.804062</td>\n",
       "      <td>-9404.738565</td>\n",
       "      <td> Spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pr_spam       pr_ham classify\n",
       "0 -3620.438622 -3628.335856     Spam\n",
       "1 -4961.487090 -5215.082391     Spam\n",
       "2  -374.143783  -393.897628     Spam\n",
       "3 -3190.000772 -3192.233073     Spam\n",
       "4 -9315.804062 -9404.738565     Spam"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hardham_classify.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the classifier on the evaluation e-mails in the ham2/spam2\n",
    "# directories.\n",
    "all_easyham2 = get_msgdir(easyham2_path)\n",
    "all_hardham2 = get_msgdir(hardham2_path)\n",
    "all_spam2 = get_msgdir(spam2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ham     0.979286\n",
       "Spam    0.020714"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The classifier does a great job on easy ham.\n",
    "easyham2_classify = spam_classifier(all_easyham2)\n",
    "class_stats(easyham2_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spam    0.693548\n",
       "Ham     0.306452"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But it does a pretty bad job on hardham,\n",
    "# not surprisingly.\n",
    "hardham2_classify = spam_classifier(all_hardham2)\n",
    "class_stats(hardham2_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spam    0.969936\n",
       "Ham     0.030064"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's also very accurate for spam.\n",
    "spam2_classify = spam_classifier(all_spam2)\n",
    "class_stats(spam2_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are are almost identical to results using the authors' R\n",
    "# script after modifying the classify.email() function to use log\n",
    "# probabilities.\n",
    "#\n",
    "#                NOT SPAM       SPAM\n",
    "# easyham2.col 0.97928571 0.02071429\n",
    "# hardham2.col 0.30241935 0.69758065\n",
    "# spam2.col    0.03006442 0.96993558"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
